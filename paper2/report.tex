\documentclass[sigconf]{acmart}

\usepackage{todonotes}
\usepackage{hyperref}

\usepackage{endfloat}
\renewcommand{\efloatseparator}{\mbox{}} % no new page between figures

\usepackage{booktabs} % For formal tables

\settopmatter{printacmref=false} % Removes citation information below abstract
\renewcommand\footnotetextcopyrightpermission[1]{} % removes footnote with conference information in first column
\pagestyle{plain} % removes running headers

\begin{document}
\title{Sociological Qualitative Methods Using Big Data}


\author{Jeramy Townsley}
\orcid{1234-5678-9012}
\affiliation{%
  \institution{IUPUI}
  \streetaddress{425 University Ave}
  \city{Indianapolis} 
  \state{Indiana} 
  \postcode{46202}
}
\email{jtownsle@indiana.edu}


\begin{abstract}
Qualitative research has been and continues to be an important research approach in the social sciences.  Some of the most important qualitative approaches are ethnography, surveys, and textual analysis, with data coding being a fundamental analytic process.  Sociology has used qualitative research to describe interactions between individuals (the micro-level), community and institutional-level dynamics (the meso-level), and structures and processes of entire societies (the macro-level).  Big data may have the unique potential to allow researchers to look at all three of these layers together in combination, in ways that have not been possible before. Similarly, big data has been used to combine qualitative and quantitative research in novel ways.  Uniquely, big data gives researchers the ability to get large amounts of real-time and ongoing data about human behavior as it exists in actual social situations, unfiltered by the somewhat artificial instruments designed by researchers.
\end{abstract}


\keywords{i523, hid347, sociology, qualitative methods, coding, ethnography, surveys}

\maketitle

\section{Introduction}
The social sciences have long been interested in collecting and analyzing data.  Research in these disciplines are often dichotomized as qualitative and quantitative, although efforts of the last couple of decades have argued for a blending of both approaches, called mixed methods, or triangulation.  Quantitative research uses various types of numerical data, employing regression, ANOVA, and other statistical analyses, to create models of social behavior, almost exclusively to test hypotheses in a deductive approach.  Qualitative research, on the other hand, uses narrative data to describe social phenomenon for a more inductive approach, developing new theoretical models, rather than testing existing theories. \cite{bryman16} While it is more intuitive to associate big data with quantitative research, there has been wide interest in applying big data to qualitative approaches.

In more recent terminology, everything produced from qualitative forms of research are called documents, which become the basis of narrative analysis.  Ethnography involves long-term immersion into a different culture for a {\em thick} description of what it is like to be a member of that community.  Surveys involve either open-ended or structured questions of research subjects, and are typically far more focused on a specific issue than ethnography, and can be far cheaper and quicker to deploy and analyze than ethnography.  Open-ended surveys are the most informal, with the researcher and participant having a significant amount of flexibility to explore wherever the conversation leads, while structured surveys have a set of predetermined questions that are rigidly followed.  Both ethnography and surveys involve narrative reports about the experiences, beliefs or thoughts of the research subjects, and build on a history of textual analysis in both the humanities and social sciences. \cite{bryman16}, \cite{singleton18}

\section{Ethnography}
Ethnography, literally, `a writing about a people,' is classically the domain of anthropologists, although sociologists have also adopted this approach.  It involves embedding oneself into a community, often for years, and doing a combination of participant observation and interviews.  This is recorded in extensive daily field notes.  Interviews are expected to be recorded, preferably with video.  These videos are transcribed word-for word.  Not only the words themselves, but also any affectations, such as pauses, saying `ummm,' repeated body movements, etc.  There is specialized software, such as NVivo, MaxQDA, and Atlas.ti, that can aid in this process of transcription, converting notes and interviews into data that can be analyzed. \cite{hand14} This process is very intensive, and time-consuming, generating an enormous amount of data.  For example, a recent popular ethnography of families facing eviction in Milwaukee, {\em Evicted}, sociologist Matthew Desmond describes the process of transcribing several years' worth of field research produced over five-thousand single-spaced notes.  \cite{desmond16}  This did not include images that were also part of the project.  

However, despite the very large amount of data this type of research produces, we have historically not considered this to be {\em big data}, as such.  If this amount of data does not constitute {\em big data}, then it becomes an important question to create parameters around the term. Hand and Hillyard review a commonly used understanding of big data, including features such as volume, velocity, variety, scope, flexibility and relationality. \cite {hand14} In the recent past, the type of document that Desmond would have created after compiling his field notes certainly would have had volume, variety and scope.  However, it would likely have simply been a large pile of typed papers, and thus would be static and non-relational.  Hand and Hillyard discuss this issue as a liminal condition because of, on the one hand, the emerging and competing discourses of what constitutes big data, and on the other hand, the changing nature of the process of ethnographic production.  Specifically, they highlight the way that big data is inherently digital, compared to the way sociology has traditionally been conceptualized.  

For example, ethnography itself is fundamentally a document produced by long-term personal interaction with community members, and is based on the researcher's personal experiences.  The contemporary researcher will almost always digitize the results of the field work.  Therefore, while old-style ethnography would have produced static, non-relational typed field notes as an intermediary product for analysis, today all of those notes would be put into a database that is flexible and relational.  This then begs the question of whether Desmond's intermediary work product is big data.  Not only did Desmond compile his field notes and interviews into digital form, but he also implemented a number of community surveys and combined that analysis with Census data. \cite{desmond16} Bringing together all of these varieties of data, from numerous sources, gets these documents closer to the way big data has been defined.  In emphasizing the digital nature of modern ethnographic processes, and the way that narrative analysis software makes qualitative research flexible and relational in a way not possible just decades ago, they argue that the modern ethnographer involved in long-term projects, like Desmond's, may constitute a big data approach to social science research. \cite{hand14}, \cite{japec15}, \cite{lazer17}

\section{New Qualitative Approaches}
Tinati, et al, make a similar argument about the importance of the digital revolution in how qualitative research is done in the social sciences. \cite{tinati14}  But rather than emphasize traditional research approaches, they look at new forms of social relationships and the types of data that are being produced, specifically, Twitter communications.  They contrast typical qualitative methods, such as ethnography and interviews, which produce a one-time data set filtered through the production matrix of the social scientists who created and implemented the original research, big data has the advantage of being real-time, on-going, and unfiltered, i.e., having a record of a complete form of behavior in the wild, as such.  A tweet, or a conversation of tweets, constitute this type of social relation that can be captured, stored and analyzed as big data. \cite{felt16}  Sociology is familiar with some aspects of this type of analysis, since it at root is simply a type of qualitative research if one just looks at individual tweets or a conversation.  However, what makes research on social media conversations fundamentally different are the same issues referred to by Hand and Hillyard \cite{hand14}--there is a possibility of real-time, and on-going behavioral analysis of people as they naturally interact in such situations. 

Referring to this dichotomy of new digital social media formats but old sociological qualitative research methods, Tinati, et al, clarify that most sociology research using Twitter to date has been this old-style, {\em small data} content analysis, using small samples of Twitter users and responses, doing classic discourse-types of investigations.  They propose looking at the structure and nature of Twitter itself as a new way to approach research, bringing the analytic approach into closer alignment with big data modalities.  For example, given that Twitter produced dynamic, on-going conversations between many people, they discuss applying a network analysis to the Twitter content. \cite{felt16} One of the classic debates within sociology is the macro-micro divide, which describes a focus on either an entire social structure, or interactions between individuals.  Both seem to provide us with information about how societies function, but it has been difficult to bridge the divide between these two types of information.  

The example they provide in their research is based on software they developed to use network analysis to look at nodes of information in conversations during a several week period in 2011 when a series of demonstrations were occurring in London. \cite{tinati14} In fact, this is the period when Occupy Wall Street was in full swing in New York City, and similar demonstrations were occurring all over the world--this analysis was of the London outgrowth of this anti-austerity movement, specifically focused on an increase of university fees.  The analysis produced a series of fairly typical network analysis diagrams, but used Twitter feeds instead of individual human contacts normally associated past sociological research using network approaches.  Whereas traditional approaches might involve several hundred people at a maximum, down to just tens of people, this analysis arose from almost 15,000 tweets of over 4,700 users.  In addition, the authors performed dynamic time evolution diagrams, together with classic discourse analysis of individual tweets.  While narrative analysis is at root a qualitative endeavor, mixed methods approaches also incorporate quantitative, statistical analysis.  This is one such study.  Network analysis is heavily mathematical as well as visual, in far more complicated ways than typical scatter plots. \cite{tinati14}

Some have argued that macro and micro are related in emergent ways, for example, while physics, chemistry and biology each have distinct theories and research methods associated with them, each represents a layer on which the next layer is built--biology emerges from chemistry, which emerges from physics. \cite{sawyer07}  While not all sociologists agree with this assessment, it is one way to discuss the fairly radical differences between these approaches to sociology.  Tinati, et al, propose that their big data approach to Twitter, using a network analysis approach, may be a bridge between the two poles, since they are able to look at the structure of a massive number of connections in a system, as well as looking at the messages from individual people. \cite{tinati14} 

\section{Qualitative Data Coding}
In computer science, coding generally refers to the process of generating statements in a specific programming language that create a specific piece of software.  In the social sciences, coding has a completely different meaning.  Referring back to the documents that are created from ethnographic research, interviews, or other type of transcribed events, the researcher has to find a way to categorize and analyze the contents.  This usually involves looking for patterned responses that can be categorized for formal analysis.  Specific words, phrases, symbols, or other types of expressions are assigned a value that are recorded into a codebook that catalogues all of the patterns that were found and deemed relevant. \cite{singleton18}, \cite{bryman16}

As already described, the micro level represents individual interactions, and macro represents all-encompassing social structures.  However, a mid-range level, meso-processes, are intermediate structures that are larger than micro, but smaller than macro.  As described by Bail, most sociological research focuses on micro to meso level dynamics, or meso to macro level dynamics, but rarely cover all three, largely confined by the scope of data that is available. \cite{bail14} For example, a micro-focused study that looks at individual actors will typically contextualize them in a {\em field} of other people in the same community or neighborhood, which would constitute the meso-level.  Such studies rarely incorporate macro-level analysis.  Similarly, a macro-focused project, such as an examination of how the U.S. history of racism combined with the 1970s rise of neoliberalism, representing macro-level dynamics, combine to produce institutional practices, the meso-level, that continue to disenfranchise race minorities, even though formal discrimination based on race has been made illegal in most transactions.  Such a study would typically not include micro-level data.  The nature of big data may allow a connection between all three levels, which is what Bail proposes.  In his survey of research on cultural sociology of the last decade, he points to the use of big data in a number of projects that seem to be able make these types of cross-level connections, and much of it relates to the development of software to access digital resources for analysis of large amounts of on-going and real-time data from a variety of sources.  He explicitly refers to the original foundations of best-practices coding analysis developed for early qualitative research, now leveraged for big data analysis. \cite{bail14}

In a final mention of recent Twitter analysis, Karamshuk, et al, developed software that performs what they call semi-automatic coding. \cite{karamshuk17} As was the case with Tinati, et al, previously discussed, the need to create software for analysis of data seems to be a common requirement of new big data approaches in the social sciences.  Given the digital form of big data, this new step in the research process would seem intuitive.  The goal of this research was to look at the public response to high-profile suicide deaths, employing modifications of classic research approaches to adapt to the big data environment.  Like Tinati, these researchers believed their approach represents a way to bridge different social levels, including micro, meso, and macro, with classic discourse analysis performing the work for the micro-level, a crowdsourcing platform for the meso-level, and finally machine-learning for modelling the macro-level structures.  At the micro-level, the authors explored appropriate language to code for emotional responses after the suicide of a celebrity or other well-known public figure.  At this level, the words themselves as expressions of the subject's interiority is a relevant data point.  However, emotions are cultural phenomenon too--our emotions are shaped profoundly by cultural patterns, informing how we should respond to events, what kinds of expressions are appropriate, etc.  These meso-level patterns that frame how we connect external events to individual-level responses were modelled as `public empathy.'  Their early attempts used hired workers to code a base of 12,000 response tweets, and then match those with the software codes, with fairly poor matches.  However, using decision tree approaches, the semi-automated response became far more accurate at correctly labelling tweets.  This was then scaled up to over a million tweets for macro-level analysis, exploring the volume and timing of expressions of various emotions in response to these types of events.  \cite{karamshuk17}

\section{Surveys and Textual Analysis}
Surveys and textual analysis are two forms of qualitative data that are often coded into forms that can be analyzed with statistical methods.  Surveys have long been a mainstay of sociological, political, and marketing research, in the form of a carefully generated set of questions that can be administered to a population, asking their opinions about various topics, or descriptions about their behavior.  While surveys that employ Likert scales, simple agree-disagree type of questions, are easy to convert into numerical form for analysis, longer-form surveys that involve open ended questions, can require the coding process. \cite{bryman16}, \cite{lazer17} Computers initially made this process far more efficient since hand-written surveys could be entered into a statistical package like SAS or R for analysis.  Similarly, even open-ended surveys that could be easily coded could then be entered for analysis.  But now, with the almost ubiquitous access in many communities to social media or e-mail, large-scale dissemination of surveys can produce tremendous amounts of data fairly rapidly. \cite{couper17}

In 2015 the American Association for Public Opinion (AAPOR) published a report of a task force on the use of surveys in the era of big data. \cite{japec15}  In it they describe a range of issues, such as ethical and policy concerns, philosophy of science differences, new methods of survey deployment and analysis, computer personnel required to implement services, interactions of how users interact with technology, and future directions for research.  One of the questions the report raises is whether surveys can survive in the new era.  While surveys are generated beforehand to gather specific kinds of information, big data is often {\em found} data already existing in the real world, and thus can represent a far more accurate picture of what people think and do.  Further, scientific surveys are typically not simply about getting opinions, but there are specific hypotheses that surveys are expected to help the researcher test.  The information big data is often providing is considered by some to be a-theoretical, describing reality itself.  This is not accepted by all, but proponents of this approach pose a challenge to traditional ways of thinking about the nature of research. \cite{japec15}, \cite{couper17}

Finally, textual analysis is another longstanding research method used in the social sciences.  Not only is textual analysis involved in data collected directly from interviews or surveys, but from found documents.  In this sense, the {\em found} nature of big data is not necessarily new to the social sciences.  Historians and literature scholars have pioneered methods of analyzing texts that were previously written, whether for public consumption, private record-keeping, or produced for specific audiences.  Rhetorical analysis, conversation analysis, and discourse analysis are all common approaches to studying narratives found in texts. The details of these various approaches are outside the scope of this present work, but they are the most archetypically qualitative, in that they are less likely to reduce the narratives into a format amenable for advanced statistical analysis. \cite{bryman16}, \cite{sagi14}  These are also the least likely to use big data approaches, since they, so far, require a human brain to carefully consider meanings of language usage.

On the other hand, several approaches use texts in a way that employs big data analysis by converting mass digitized texts into components amenable to mathematical dissection.  This approach is well-represented by studies that use digitized textual archives to search for patterns of word usage to indicate social trends.  One example is a study by Chen and Yan, who use Google's N-gram corpus to analyze the way various sociological words were used through a time-period of about 150 years. \cite{chen16}  At the time of this study this resource had over eight-million books in the archive, and each word from these books was indexed into a database, which is the `N-gram' feature, a count of these words.  The authors analyzed various sociologically-related words, people, theories, and research methods. They draw various conclusions based on these different analyses, including the fact that older generations of sociologists had the possibility for a higher citation rate and their works had longer life-spans, even when controlling for their longer existence in the body of literature.  This may be because of a vastly larger market of sociologists, and a faster moving set of ideas compared to a century ago.  Related to this present work, they look at time dynamics for qualitative versus quantitative research, finding that quantitative analysis has dominated sociology since 2000, but that qualitative researchers are more likely to publish books rather than articles. \cite{chen16}

A second example of a textual analysis is from an approach to study moral cultures, by Vaisey and Miles. \cite{vaisey14}  After summarizing a variety of small-scale approaches to coding language usage in specific groups and linking word usage to sociological and psychological theories of values, the authors propose scaling up this approach to sources available on the internet, since the internet itself can be described as one large series of documents that can be subjected to text analysis.  The core of their approach relies on software such as  Linguistic Inquiry and Word Count (LIWC), that allows the user to input documents and it extracts word counts of the researcher's choosing.  In this case specific words are mapped onto how they have previously been demonstrated to relate to specific cultural values.  The more certain words are used by different groups, it implies that they rank higher or lower on different values.  For example, the value of power uses terms related to social status, prestige and dominance over others, while the value of benevolence uses words related to the welfare of others.  A large list of specific words is catalogued for empirical use in the Moral Foundations Dictionary (MFD).  \cite{vaisey14}  

As a specific of this type of analysis, Sagi and Dehgani use a corpus of New York Times archives to analyze the way the newspaper talked about the World Trade Center (WTC) during various time-periods. \cite{sagi14} This archive contains 1.8 million articles from 1987 to 2008, and 5.8 billion words.  Using the LIWC, and a list of morality-related words from the MFD, they found that moral rhetoric changed in response to the first WTC attack in 1993, and then again after the two towers were felled in 2001.  They controlled for architectural moral rhetoric by the newspaper by comparing language used about the Empire State Building.  While the latter had a fairly stable usage of language, there were dramatic and lasting changes after both terrorist attacks on the WTC.  All of the five specific moral values (authority, fairness, harm, purity, loyalty) significantly and substantially increased, but harm and loyalty increased the most for both attacks.

\section{Conclusion}
Qualitative research has been and continues to be an important research modality in the social sciences and humanities.  Sociology has used qualitative research to describe interactions between individuals (the micro-level), community and institutional-level dynamics (the meso-level), and structures and processes of entire societies (the macro-level).  Big data may have the unique potential to allow researchers to look at all three of these layers together in combination, in ways that have not been possible before. Similarly, big data has been used to combine qualitative and quantitative research in novel ways.  Uniquely, big data gives researchers the ability to get large amounts of real-time and ongoing data about unstructured human behavior 'in the wild,' as such.  This is in contrast to the traditional practice of creating a survey with a limited number of questions that is deployed in an artificial setting, which itself may create a picture of society that does not necessarily match reality.  There are also limitations to these approaches in studying past events, in that the researcher is limited to the subject's memories.  Big data can pull vast amounts of real-time experiences as they happened, since a wired society is expressing themselves on social media, thus creating a data set for sociological analysis.  Software like the LIWC, combined with the Moral Foundations Dictionary, can allow a researcher to assess how the public is using morally rhetorical language in response to specific events as they happen.


\bibliographystyle{ACM-Reference-Format}
\bibliography{report} 

\end{document}
